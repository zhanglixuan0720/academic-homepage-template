---
title: "eLabrador"
description: "A wearable navigation system for blind and visually impaired"
date: "2023-09-01"
image: "/eLabrador.png"
---

## A wearable navigation system for blind and visually impaired

We present FibeRobo, a thermally-actuated liquid crystal elastomer (LCE) fiber that can be embedded or structured into textiles and enable silent and responsive interactions with shape-changing, fiber-based interfaces. Three definitive properties distinguish FibeRobo from other actuating threads explored in HCI. First, they exhibit rapid thermal self-reversing actuation with large displacements (âˆ¼40%) without twisting. Second, we present a reproducible UV fiber drawing setup that produces hundreds of meters of fiber with a sub-millimeter diameter. Third, FibeRobo is fully compatible with existing textile manufacturing machinery such as weaving looms, embroidery, and industrial knitting machines. This paper contributes to developing temperature-responsive LCE fibers, a facile and scalable fabrication pipeline with optional heating element integration for digital control, mechanical characterization, and the establishment of higher hierarchical textile structures and design space. Finally, we introduce a set of demonstrations that illustrate the design space FibeRobo enables.

Blind and visually impaired people almost cannot walk outdoors, especially in unfamiliar environments. Without adequate assistance, the quality of their life is seriously affected. To improve the quality of the blind's life, this article presents 

We present eLabrador, a wearable navigation system for navigating the blind walk outdoors, e.g. from the residential entrance to nearby wayside park. 
The hardware and software of our proposed system are designed jointly, consisting of three successive modules: Firstly, the perception module adopts the head-mounted RGBD camera to perceive the surroundings, and the Inertial Measurement Unit (IMU) to capture the ego-motion, generating a 3D semantic map of the surrounding environment. Afterwards, the planning module further generates a precise walking path, utilizing the 3D semantic map and referencing the global Amap. Finally, the interaction module exploits the audio-haptic hybrid feedback to convey navigation information to blind and visually impaired people. The overall three modules work together to navigate blind and visually impaired people walking in real time. Experimental results and analysis in real-world scenarios on 10 subjects demonstrate that our system can successfully and safely navigate users walking outdoors, improving their mobility.
